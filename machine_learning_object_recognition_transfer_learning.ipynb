{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_learning_object_recognition_transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shellerbrand/machine-learning-for-object-recognition/blob/update-lmwl-2020/machine_learning_object_recognition_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r73kWA5UqGQH"
      },
      "source": [
        "# Transferlernen - Objekterkennung mit Neuronalen Netzen\n",
        "\n",
        "Das Erkennen von Objekten ist eine Aufgabe, die der Mensch ausgezeichnet beherrscht. Auch Computer sind inzwischen mit Hilfe von künstlichen neuronalen Netzen in der Lage, diese Aufgabe in spezifischen Fällen sehr zuverlässig zu erledigen.\n",
        "\n",
        "Besonders leistungsfähig is hierbei die Klasse der Convolutional Neural Networks (ConvNets). Diese Netze bestehen aus einer Folge von Schichten. Die Ausgänge der ersten Schichten entsprechen dabei der Bildzusammensetzung auf Pixel-Ebene, wohingegen die Ausgänge späterer Schichten immer großflächigere Zusammenhänge beschreiben.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM3YyqJ9qBu2"
      },
      "source": [
        "##  Vorarbeiten\n",
        "\n",
        "Diese Schritte müssen nur einmal ausgeführt werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoBJmgpEZM9u",
        "cellView": "form"
      },
      "source": [
        "#@title Das Machine Learning Framework (PyTorch) laden \n",
        "# http://pytorch.org/\n",
        "\n",
        "# Vision fokussierten Teil von PyTorch\n",
        "!pip install torchvision\n",
        "!pip install pillow==4.1.1\n",
        "\n",
        "# Plotting etc imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "# Zufallszahlen-Initialisierung fuer das Machine Learning Framework\n",
        "seed=2\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic=True\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "seaborn.set_style(\"white\")\n",
        "plt.rcParams['figure.dpi'] = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPtNMo4rmGPc",
        "cellView": "form"
      },
      "source": [
        "#@title Hilfsfunktionen für Training and Visualisierung laden\n",
        "\n",
        "# \n",
        "# Funktionen um Bilder zu laden vom Image-Upload Tool\n",
        "#\n",
        "\n",
        "print('Laden von Funktionen für das Laden der Trainings- und Evaluations-Bilder.')\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "HOST = 'https://imageupload.hellerbit.com'\n",
        "API_KEY = 'bce8a814f0b3882ac6f2810464128b42dbfee105e1ba84311a77b3f4'\n",
        "DIR = './tmp/'\n",
        "TRAINING = 'training'\n",
        "EVALUATION = 'evaluation'\n",
        "EXP_TRAIN_A = '1_training_kategorie_A'\n",
        "EXP_TRAIN_B = '1_training_kategorie_B'\n",
        "EXP_EVALUATION_A = '1_evaluation_kategorie_A'\n",
        "EXP_EVALUATION_B = '1_evaluation_kategorie_B'\n",
        "OUTDIR = './output/'\n",
        "\n",
        "REGEX_CAT_EXPERIMENT = re.compile('.*_kategorie_(.*)')\n",
        "\n",
        "\n",
        "if not os.path.isdir(DIR):\n",
        "    print('Making the directory: {}'.format(DIR))\n",
        "    os.makedirs(DIR)\n",
        "\n",
        "def clear_dir():\n",
        "    expdir = os.path.join(DIR, TRAINING)\n",
        "    if os.path.isdir(expdir):\n",
        "        shutil.rmtree(expdir)\n",
        "    expdir = os.path.join(DIR, EVALUATION)\n",
        "    if os.path.isdir(expdir):\n",
        "        shutil.rmtree(expdir)\n",
        "\n",
        "def download_experiment_images(experiment_id, trainoreval):\n",
        "  \n",
        "    cat = REGEX_CAT_EXPERIMENT.search(experiment_id).group(1)\n",
        "    \n",
        "    \n",
        "    url = ''.join([HOST, '/api/{}/all.txt'.format(experiment_id)])\n",
        "    headers = {'APIKEY': API_KEY}\n",
        "    resp = requests.get(url, headers=headers)\n",
        "\n",
        "    print(resp.text)\n",
        "    files = resp.text.split('\\n')\n",
        "    # Make the directory for the experiment\n",
        "    expdir = os.path.join(DIR, trainoreval, cat)\n",
        "    if not os.path.isdir(expdir):\n",
        "            os.makedirs(expdir)\n",
        "\n",
        "    for line in files:\n",
        "        print(\"Loading this file: {}\".format(line))\n",
        "        url = ''.join([HOST, '/api/{}/{}'.format(experiment_id, line)])\n",
        "        path = os.path.join(expdir, line)\n",
        "        print(\"\\t from URL {}\".format(url))\n",
        "        r = requests.get(url, stream=True, headers=headers)\n",
        "        if r.status_code == 200:\n",
        "            with open(path, 'wb') as f:\n",
        "                for chunk in r:\n",
        "                    f.write(chunk)\n",
        "                    \n",
        "\n",
        "print('Laden von Funktionen für das Training und für die Visualisierung.')\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoche {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in [TRAINING, EVALUATION]:\n",
        "            if phase == TRAINING:                \n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == TRAINING):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == TRAINING:\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Fehler: {:.4f} Genauigkeit: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == TRAINING:\n",
        "              scheduler.step()\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == EVALUATION and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training abgeschlossen in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Beste Testgenauigkeit: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders[EVALUATION]):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "#             print(torch.exp(outputs[0]))\n",
        "#             print(torch.sum(torch.exp(outputs[0])))            \n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                fig = plt.figure()\n",
        "#                 ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                plt.axis('off')\n",
        "                plt.title('Vorhersage: {}'\n",
        "                          .format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "        \n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    plt.axis('off')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    \n",
        "# Is the CUDA Accelerator available?\n",
        "print(\"Beschleunigung mit der Grafikkarte (GPU): {}\".format(torch.cuda.is_available()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buf8lzUuqPkz"
      },
      "source": [
        "## Bilder hochladen\n",
        "\n",
        "Im Folgenden werden die Bilder importiert, die für das Training der Objekterkennung benutzt werden. Es wird hier bewusst eine Kategorien mit dem Namen *A* und *B* verwendet, da man so mit verschiedenen Objektklassen experimentieren kann. In unserem Beispiel nutzen wir zwei verschiedene Arten von Süßigkeiten:\n",
        "\n",
        "- __Kategorie A__: Gummibärchen\n",
        "- __Kategorie B__: Lakritze\n",
        "\n",
        "### Trainingsbilder\n",
        "\n",
        "Die *Trainingsbilder* sind dafür da, dem Netz beizubringen, wie Objekte der jeweiligen Klasse aussehen. Die Trainingsbilder könnt Ihr z.B. mit dem Handy auf folgenden Links hochladen:\n",
        "\n",
        "- __Kategorie A__ (Gummibärchen): https://imageupload.hellerbit.com/images/1_training_kategorie_A\n",
        "- __Kategorie B__ (Lakritze): https://imageupload.hellerbit.com/images/1_training_kategorie_B\n",
        "\n",
        "### Testbilder\n",
        "\n",
        "Außerdem brauchen wir noch Bilder, mit denen die Leistungsfähigkeit gemessen werden soll, nachdem das Netz  anhand der *Trainingsbilder* gelernt hat, die zwei Klassen zu unterscheiden. Die *Testbilder* können über folgende Links hochgeladen werden:\n",
        "\n",
        "- __Kategorie A__ (Gummibärchen): https://imageupload.hellerbit.com/images/1_evaluation_kategorie_A\n",
        "- __Kategorie B__ (Lakritze): https://imageupload.hellerbit.com/images/1_evaluation_kategorie_B\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEo7crOsNcAW",
        "cellView": "form"
      },
      "source": [
        "#@title Den Schlüssel zum Download der Bilder hier eingeben\n",
        "API_KEY = \"3c7f5f9a4d227972f0989cb7c1be7ad0835990d03f84ed8efbae839a\" #@param {type:\"string\"}\n",
        "\n",
        "clear_dir()\n",
        "download_experiment_images(EXP_TRAIN_A, TRAINING)\n",
        "download_experiment_images(EXP_TRAIN_B, TRAINING)\n",
        "download_experiment_images(EXP_EVALUATION_A, EVALUATION)\n",
        "download_experiment_images(EXP_EVALUATION_B, EVALUATION)\n",
        "\n",
        "\n",
        "data_dir = DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mT6kgqKbWF_",
        "cellView": "form"
      },
      "source": [
        "#@title Bild-Daten ergänzen um Vergrößerung, Drehung, Farbnormalisierung etc.\n",
        "\n",
        "data_transforms = {\n",
        "    'training': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'evaluation': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in [TRAINING, EVALUATION]}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in [TRAINING, EVALUATION]}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAINING, EVALUATION]}\n",
        "class_names = image_datasets[TRAINING].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Es gibt folgende Klassen: {}\".format(\", \".join(_ for _ in class_names )))\n",
        "print(\"Training: {} Bilder\".format(dataset_sizes[TRAINING]))\n",
        "print(\"Test: {} Bilder\".format(dataset_sizes[EVALUATION]))\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa98J-PHvQwf"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmo-xAfOtyru",
        "cellView": "form"
      },
      "source": [
        "#@title Beispielbilder für das Training anzeigen\n",
        "n_batch = 10\n",
        "\n",
        "print('Augmented Set: {}'.format(len(dataloaders[TRAINING])))\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs = torch.cat([next(iter(dataloaders[TRAINING]))[0] for _ in range(n_batch)], dim=0)\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "# imshow(out, title=[class_names[x] for x in classes])\n",
        "imshow(out, title=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arvE4QC_mczz",
        "cellView": "form"
      },
      "source": [
        "#@title Das Model trainieren, so dass neue Kategorien erkannt werden.\n",
        "training_epochen = 7 #@param {type:\"slider\", min:1, max:25, step:1}\n",
        "\n",
        "model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opoosed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
        "\n",
        "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=training_epochen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdwCuRjgvULk"
      },
      "source": [
        "## Auswertung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwRv3Hdlm0am"
      },
      "source": [
        "visualize_model(model_conv, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikqBpzxJyfw9"
      },
      "source": [
        "---\n",
        "\n",
        "## Referenzen\n",
        "\n",
        "* Chilamkurthy, S; PyTorch Transfer Learning Tutorial, Version Dec 2018, [PyTorch Docs](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "* He, K. et al.; \n",
        "Deep Residual Learning for Image Recognition, Dec 2015, [arxiv](https://arxiv.org/abs/1512.03385) (RESNET-18 ConvNet)\n",
        "\n",
        "Der Source-Code für dieses Notebook und das Tool zum Bilder hochladen sind bzw. werden hier veröffentlicht:\n",
        "\n",
        "https://github.com/shellerbrand/machine-learning-for-object-recognition\n",
        "\n"
      ]
    }
  ]
}